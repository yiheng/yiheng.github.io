---
layout: post
title: JVM上如何进行高效的矩阵相乘
abstract: 矩阵相乘在深度学习中占据主要运行时间，其性能的好坏对于程序影响很大。这篇文章介绍如何在JVM上通过使用native的数学运算库进行高效的矩阵相乘。
tags:机器学习，Java
---
在神经网络中，矩阵相乘往往占据了70%以上的时间。矩阵相乘不仅仅被用在全连接层，在[这篇文章](http://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/)中，介绍了怎样利用矩阵相乘进行卷积运算。可以说矩阵相乘是深度学习的核心。

在不同的平台上，业界早已经有了比较成熟的高效矩阵相乘的实现，例如MKL，OpenBLAS，clBLAS，CUBLAS等。

对于矩阵相乘的性能优化，采用的方式包括使用SIMD指令、多线程和Cache访问优化。

从性能的角度，目前JVM上还没有与上面相媲美的实现。所以，通常的玩法是通过JNI调用这些native的库。

[netlib-java](https://github.com/fommil/netlib-java)就是这么玩的。它提供一组标准的线性代数运算接口(BLAS, LAPACK, ARPACK)，如果本地安装了支持这些接口的native库，它会直接使用这些高效的native库进行计算，否则使用一个JVM的版本进行计算。

目前Spark就是使用netlib-java进行线性代数运算。

矩阵相乘在netlib里的方法名称是gemm（General Matrix to Matrix Multiplication）。分成双精度（dgemm）和单精度（sgemm）两个版本，这两个版本的参数是一致的，只不过是double和float的区别。

以mkl为例，在JVM上使用netlib-java。假设intel的安装文件在/opt/intel下面

1. netlib-java会寻找blas和lapack的动态库，所以要在/usr/lib/下面创建名为libblas.so和liblapack.so的软连接。
<code>sudo ln -sf /opt/intel/mkl/lib/intel64/libmkl_rt.so /usr/lib/libblas.so</code>
<code>sudo ln -sf /opt/intel/mkl/lib/intel64/libmkl_rt.so /usr/lib/libblas.so.3</code>
<code>sudo ln -sf /opt/intel/mkl/lib/intel64/libmkl_rt.so /usr/lib/liblapack.so</code>
<code>sudo ln -sf /opt/intel/mkl/lib/intel64/libmkl_rt.so /usr/lib/liblapack.so.3</code>

2. 将mkl的lib路径加入ld.conf，使得操作系统载入mkl的动态库
<code>sudo echo -e '/opt/intel/mkl/lib/intel64\n/opt/intel/lib/intel64' >/etc/ld.so.conf.d/libblas.conf </code><br>
<code>sudo ldconfig</code><br>

3. 加入netlib-java的依赖
<dependency>
  <groupId>com.github.fommil.netlib</groupId>
    <artifactId>all</artifactId>
      <version>1.1.2</version>
        <type>pom</type>
        </dependency>

4. 代码示例



